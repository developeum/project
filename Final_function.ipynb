{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_function(csv_in, csv_out):\n",
    "    import pandas\n",
    "    import pymorphy2\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download(\"stopwords\")\n",
    "    from langdetect import detect\n",
    "    \n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    pandas.options.mode.chained_assignment = None\n",
    "    \n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    #the first function\n",
    "    \n",
    "    events_df = pandas.read_csv(csv_in)\n",
    "    events_df['normalized_name'] = events_df['name']\n",
    "    events_df['normalized_description'] = events_df['description']\n",
    "\n",
    "    idxs = []\n",
    "    for i in range(len(events_df)):\n",
    "        descr = events_df['description'][i]\n",
    "        name = events_df['name'][i]\n",
    "        if (detect(name) != 'ru' and detect(name) != 'en') or (detect(descr) != 'ru' and detect(descr) != 'en'):\n",
    "            idxs.append(i)\n",
    "    events_df = events_df.drop(idxs)\n",
    "\n",
    "    events_df['normalized_description'] = events_df['normalized_description'].replace(r'https?:\\/\\/[^\\s]+', '', regex=True)\n",
    "    events_df['normalized_description'] = events_df['normalized_description'].str.lower()\n",
    "    events_df['normalized_description'] = events_df['normalized_description'].replace('ё', 'е', regex=True)\n",
    "    events_df['normalized_description'] = events_df['normalized_description'].replace(\n",
    "        r'\\w*январ\\w*|\\w*феврал\\w*|\\w*март\\w*|\\w*апрел\\w*|\\w*июн\\w*|\\w*июл\\w*|\\w*август\\w*|\\w*сентябр\\w*|\\w*октябр\\w*|\\w*ноябр\\w*|\\w*декабр\\w*|\\bма.\\b',\n",
    "        ' ', regex=True)\n",
    "    events_df['normalized_description'] = events_df['normalized_description'].replace(r'[^а-яА-Яa-zA-Z]', ' ', regex=True)\n",
    "\n",
    "    events_df['normalized_name'] = events_df['normalized_name'].str.lower()\n",
    "    events_df['normalized_name'] = events_df['normalized_name'].replace('ё', 'е', regex=True)\n",
    "    events_df['normalized_name'] = events_df['normalized_name'].replace(\n",
    "        r'\\w*январ\\w*|\\w*феврал\\w*|\\w*март\\w*|\\w*апрел\\w*|\\w*июн\\w*|\\w*июл\\w*|\\w*август\\w*|\\w*сентябр\\w*|\\w*октябр\\w*|\\w*ноябр\\w*|\\w*декабр\\w*|\\bма.\\b',\n",
    "        ' ', regex=True)\n",
    "    events_df['normalized_name'] = events_df['normalized_name'].replace(r'[^а-яА-Яa-zA-Z]', ' ', regex=True)\n",
    "\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    stopwords = stopwords.words(\"russian\")\n",
    "    stopwords.extend(\n",
    "        ['что', 'это', 'весь', 'этот', 'привет', 'так', 'вот', 'как', 'ссылка', 'регистрация', 'приглашать', 'еще',\n",
    "         'год', 'спикер', 'вопрос', 'тема', 'наш', 'свой', 'время', 'который', 'выступить', 'встреча', 'мочь',\n",
    "         'jenkins', 'masked', 'ваш', 'epam', 'место', 'spb', 'новый'])\n",
    "\n",
    "    rus_names = list(pandas.read_csv('russian_names.csv')['Name'].values)\n",
    "    rus_surnames = list(pandas.read_csv('russian_surnames.csv')['Surname'].values)\n",
    "    for_names = list(pandas.read_csv('foreign_names.csv')['name'].values)\n",
    "    all_names = rus_names + rus_surnames + for_names\n",
    "\n",
    "    descr_array = []\n",
    "    names_array = []\n",
    "    for i in range(len(events_df)):\n",
    "        descr = events_df['normalized_description'][i]\n",
    "        if not pandas.isna(descr):\n",
    "            descr = [morph.parse(word)[0].normal_form for word in descr.split() if\n",
    "                     morph.parse(word)[0].tag.POS not in {'INTJ', 'PRCL', 'CONJ', 'PREP', 'NPRO'}\n",
    "                     and morph.parse(word)[0].normal_form not in stopwords\n",
    "                     and morph.parse(word)[0].normal_form not in all_names]\n",
    "            if not descr:\n",
    "                descr = np.nan\n",
    "            else:\n",
    "                descr = ' '.join(descr)\n",
    "        descr_array.append(descr)\n",
    "\n",
    "        name = events_df['normalized_name'][i]\n",
    "        if not pandas.isna(name):\n",
    "            name = [morph.parse(word)[0].normal_form for word in name.split() if\n",
    "                    morph.parse(word)[0].tag.POS not in {'INTJ', 'PRCL', 'CONJ', 'PREP', 'NPRO'}]\n",
    "            name = ' '.join(name)\n",
    "        names_array.append(name)\n",
    "\n",
    "    events_df['normalized_name'] = names_array\n",
    "    events_df['normalized_description'] = descr_array\n",
    "    events_df = events_df[['name', 'normalized_name', 'event_type', 'event_time', 'description', 'normalized_description', 'city', 'categories']]\n",
    "\n",
    "    #the second function\n",
    "\n",
    "    hackathon = ['hackathon', 'хакатон', 'Конкурс']\n",
    "    webinar = ['webinar', 'вебинар', 'lecture', 'лекция', 'Вебинар', 'Лекция']\n",
    "    conference = ['conference', 'конференция', 'Конференция']\n",
    "    training = ['training', 'тренинг', 'семинар', 'workshop', 'мастер класс', 'воркшоп', 'Тренинг']\n",
    "    course = [r'\\bcourse\\b', r'\\bкурс\\b']\n",
    "    meetup = ['meetup', 'митап', 'Митап']\n",
    "    olympiad = ['olympiad', 'олимпиада']\n",
    "\n",
    "    types = ['hackathon', 'webinar', 'conference', 'training', 'course', 'meetup', 'olympiad', 'others']\n",
    "    keywords = [hackathon, webinar, conference, training, course, meetup, olympiad]\n",
    "\n",
    "    online = ['Без города', 'без города', 'online', 'онлайн', 'Online', 'Онлайн', 'Internet', 'internet', 'Интернет', 'интернет']\n",
    "\n",
    "    for i in range(len(events_df)):\n",
    "        descr = events_df['normalized_description'][i]\n",
    "        name = events_df['normalized_name'][i]\n",
    "        type = events_df['event_type'][i]\n",
    "        city = events_df['city'][i]\n",
    "        if pandas.isna(city) or any(word in city for word in online):\n",
    "            events_df['city'][i] = 'online'\n",
    "        for j in range(len(keywords)):\n",
    "            if any(word in type for word in keywords[j]):\n",
    "                events_df['event_type'][i] = types[j]\n",
    "            elif any(word in name for word in keywords[j]):\n",
    "                events_df['event_type'][i] = types[j]\n",
    "            elif not pandas.isna(descr):\n",
    "                if any(word in descr for word in keywords[j]):\n",
    "                    events_df['event_type'][i] = types[j]\n",
    "    events_df['event_type'] = events_df['event_type'].fillna(types[-1])\n",
    "\n",
    "    #the third function\n",
    "    \n",
    "    clas=[]\n",
    "    ds = ['AI', 'Data Science', 'ai', 'data science', 'data engineering', 'data scientist', 'deep learning', 'машинное обучение', 'нейросети']\n",
    "    mobile =['Android', 'mobile']\n",
    "    qa =['QA', 'qa', 'тестировщик']\n",
    "    web =['Web', 'Front-end', 'frontend','фронтенд-разработчик']\n",
    "    devops = ['DevOps', 'devops']\n",
    "    busorg = ['BA', 'Business', 'HR', 'Marketing', 'hr', 'project manager', 'менеджмент']\n",
    "\n",
    "    possible_class = ['ds', 'mobile', 'qa', 'web', 'devops', 'busorg']\n",
    "    \n",
    "    categories = events_df['categories'].to_numpy().astype(str)\n",
    "    \n",
    "    for x in categories:\n",
    "        count =[]\n",
    "        mask = np.isin(x.split(','), ds)\n",
    "        count.append(np.count_nonzero(mask == bool(\"True\")))\n",
    "\n",
    "        mask = np.isin(x.split(','), mobile)\n",
    "        count.append(np.count_nonzero(mask == bool(\"True\")))\n",
    "\n",
    "        mask = np.isin(x.split(','), qa)\n",
    "        count.append(np.count_nonzero(mask == bool(\"True\")))\n",
    "\n",
    "        mask = np.isin(x.split(','), web)\n",
    "        count.append(np.count_nonzero(mask == bool(\"True\")))\n",
    "\n",
    "        mask = np.isin(x.split(','), devops)\n",
    "        count.append(np.count_nonzero(mask == bool(\"True\")))\n",
    "\n",
    "        mask = np.isin(x.split(','), busorg)\n",
    "        count.append(np.count_nonzero(mask == bool(\"True\")))\n",
    "\n",
    "        if np.count_nonzero(count)  > 0:\n",
    "            ind = np.argmax(count)\n",
    "            clas.append(possible_class[ind])\n",
    "        else:\n",
    "            clas.append('undefined')\n",
    "            \n",
    "    cl = pd.DataFrame(clas, columns=['class_tmp'])\n",
    "    \n",
    "    temp_df = events_df.join(cl, how='inner')\n",
    "    \n",
    "    pkl_filename = \"pickle_model_ada.pkl\"\n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        pickle_model = pickle.load(file)\n",
    "    \n",
    "    pkl_vec_filename = \"pickle_vect.pkl\"\n",
    "    with open(pkl_vec_filename, 'rb') as file:\n",
    "        pickle_vec = pickle.load(file)\n",
    "        \n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    \n",
    "    label = temp_df['class_tmp'].to_numpy()\n",
    "    desc =  temp_df['normalized_description'].to_numpy()\n",
    "\n",
    "    description = pickle_vec.transform(desc)\n",
    "    \n",
    "    class1 =[]\n",
    "    \n",
    "    i = 0\n",
    "    for x in label:\n",
    "        if x == 'busorg':\n",
    "            class1.append('busorg')\n",
    "        elif x == 'devops':\n",
    "            class1.append('devops')\n",
    "        elif x == 'ds':\n",
    "            class1.append('ds')\n",
    "        elif x == 'mobile':\n",
    "            class1.append('mobile')\n",
    "        elif x == 'qa':\n",
    "            class1.append('qa')\n",
    "        elif x == 'web':\n",
    "            class1.append('web')\n",
    "        else:        \n",
    "            proba = pickle_model.predict_proba(description[i])\n",
    "            proba = proba*100\n",
    "            print(proba)\n",
    "            if max(proba[0] > 49.9):\n",
    "                print(np.argmax(proba[0]))\n",
    "                if np.argmax(proba[0]) == 0:\n",
    "                    class1.append('busorg')\n",
    "                elif np.argmax(proba[0]) == 1:\n",
    "                    class1.append('devops')\n",
    "                elif np.argmax(proba[0]) == 2:\n",
    "                    class1.append('ds')\n",
    "                elif np.argmax(proba[0]) == 3:\n",
    "                    class1.append('mobile')\n",
    "                elif np.argmax(proba[0]) == 4:\n",
    "                    class1.append('qa')\n",
    "                elif np.argmax(proba[0]) == 5:\n",
    "                    class1.append('web')\n",
    "                else:\n",
    "                    class1.append('other')\n",
    "            else:\n",
    "                class1.append('other')\n",
    "           \n",
    "        i = i + 1\n",
    "    \n",
    "    new_class = pd.DataFrame(class1, columns=['class'])\n",
    "    final_df = temp_df.join(new_class, how='inner')\n",
    "    final_df = final_df.drop(['class_tmp'], axis=1)\n",
    "    \n",
    "    final_df.to_csv(csv_out, index=False, index_label=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Мищенков\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f9689b26b14b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mFinal_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/events_crawlers_02112020.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/Some_random_data_result.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/data/Some_random_data_result.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-90d6618e26fa>\u001b[0m in \u001b[0;36mFinal_function\u001b[1;34m(csv_in, csv_out)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mnames_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevents_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mdescr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevents_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'normalized_description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             descr = [morph.parse(word)[0].normal_form for word in descr.split() if\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Final_function('data/events_crawlers_02112020.csv', 'data/Some_random_data_result.csv')\n",
    "file = pd.read_csv('data/data/Some_random_data_result.csv', sep=',')\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
